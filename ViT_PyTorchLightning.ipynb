{"cells":[{"cell_type":"markdown","metadata":{"id":"04gG8tP91w2Q"},"source":["# Mount"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guNmtsUJfvUp"},"outputs":[],"source":["def mount(gpu_info=True, ram_info=True):\n","    # Googleドライブをマウント\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    if gpu_info:\n","        # GPUの割り当てを確認\n","        gpu_info = !nvidia-smi\n","        gpu_info = '\\n'.join(gpu_info)\n","        if gpu_info.find('failed') >= 0:\n","            print('Not connected to a GPU')\n","        else:\n","            print(gpu_info)\n","\n","    if ram_info:\n","        # 使用可能なメモリ量を確認\n","        from psutil import virtual_memory\n","        ram_gb = virtual_memory().total / 1e9\n","        print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","        if ram_gb < 20:\n","            print('Not using a high-RAM runtime')\n","        else:\n","            print('You are using a high-RAM runtime!')"]},{"cell_type":"markdown","metadata":{"id":"Ed0SIekj4Zrf"},"source":["# Installing module"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21729,"status":"ok","timestamp":1702711497558,"user":{"displayName":"nota tech","userId":"08868539642021101661"},"user_tz":-540},"id":"kwXqxQf4rbdH","outputId":"d7c07b22-c03c-448f-c652-cc8f4b62f735"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# インストール\n","!pip install -q pytorch_lightning\n","!pip install -q torchmetrics\n","!pip install --upgrade -q wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ufwTt-N4NO2"},"outputs":[],"source":["import os\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from PIL import Image\n","import copy\n","\n","# Pytorch\n","import torch\n","from torch import nn, optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision\n","import torchvision.transforms as T\n","\n","# PytorchLightning\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","from torchmetrics import Accuracy\n","from pytorch_lightning.callbacks import Callback\n","from pytorch_lightning import seed_everything\n","from pytorch_lightning.loggers import WandbLogger\n","\n","# ignore warning\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"GTZIzmhT5VZi"},"source":["# Create Data"]},{"cell_type":"markdown","source":["* データはWandBのアーティファクトを使用"],"metadata":{"id":"M0xay9FeVH6g"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4N9V71t9u10"},"outputs":[],"source":["def download_data():\n","    # v3は教師データ増の為にdfのカラム増やしており、念のためv2のままとする（元々v2のバージョンでやっていた）\n","    processed_data_at = wandb.use_artifact(f'{PROCESSED_DATA_AT}:v2', type='split_data')\n","    processed_dataset_dir = Path(processed_data_at.download())\n","    # v0\n","    processed_data_at_v0 = wandb.use_artifact(f'{PROCESSED_DATA_AT}:v0', type='split_data')\n","    processed_dataset_dir_v0 = Path(processed_data_at_v0.download())\n","\n","    return processed_dataset_dir, processed_dataset_dir_v0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJzyl3vQ-LI0"},"outputs":[],"source":["def get_df(config, processed_dataset_dir, processed_dataset_dir_v0, data_reduction=False):\n","    df = pd.read_csv(processed_dataset_dir_v0 / 'data_split.csv')\n","\n","    # 画像パスの割り当て\n","    df[\"image_fname\"] = [processed_dataset_dir/f'images/{f}' for f in df.File_Name.values]\n","\n","    # ラベルの割り当て\n","    def get_label(row):\n","        for key, value in STL10_CLASSES.items():\n","            if row[value] == 1:\n","                return key  # 整数のキーを返す\n","        return None\n","\n","    df['label'] = df.apply(get_label, axis=1)\n","\n","\n","    \"\"\"\n","    データを削減する（動作確認の時用）\n","    \"\"\"\n","    if data_reduction:\n","        # train, valid, testごとにランダムに batch_size*3ずつ選択\n","        df_train = df[df['Split_add_valid'] == 'train'].sample(n=config[\"batch_size\"]*3, random_state=1)\n","        df_valid = df[df['Split_add_valid'] == 'valid'].sample(n=config[\"batch_size\"]*3, random_state=1)\n","        df_test = df[df['Split_add_valid'] == 'test'].sample(n=config[\"batch_size\"]*3, random_state=1)\n","\n","        # 選択した行を結合\n","        df = pd.concat([df_train, df_valid, df_test]).reset_index(drop=True)\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTRvMxLP4Enr"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, df, transform=None):\n","        self.df = df\n","        self.transform = transform\n","        self.classes = [STL10_CLASSES[i] for i in range(len(STL10_CLASSES))]\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.df.iloc[idx, -2]  # 画像のパス\n","        image = Image.open(img_name).convert('RGB')\n","\n","        label = self.df.iloc[idx, -1]  # ラベル\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TsfmFHL34Elr"},"outputs":[],"source":["def create_transforms(img_size, mean, std):\n","    return {\n","        \"train\": T.Compose(\n","            [\n","                T.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n","                T.RandomHorizontalFlip(),\n","                T.ToTensor(),\n","                T.Normalize(mean=mean, std=std),\n","            ]\n","        ),\n","        \"valid\": T.Compose(\n","            [\n","                T.ToTensor(),\n","                T.Normalize(mean=mean, std=std),\n","            ]\n","        ),\n","        \"test\": T.Compose(\n","            [\n","                T.ToTensor(),\n","                T.Normalize(mean=mean, std=std),\n","            ]\n","        )\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4HZfeVuzz9FC"},"outputs":[],"source":["def print_dataloader_info(dl, name=\"DataLoader\"):\n","    print(f\"--- {name} ---\")\n","    print(f\"Number of batches: {len(dl)}\")\n","    print(f\"Total number of items: {len(dl.dataset)}\")\n","    print(\"---------------\")\n","\n","def show_batch_with_title(dl, title, max_n=16, figsize=(5, 5)):\n","    print(title)\n","    images, _ = next(iter(dl))\n","    grid_img = torchvision.utils.make_grid(images[:max_n], nrow=4, normalize=True)\n","    plt.figure(figsize=figsize)\n","    plt.imshow(grid_img.permute(1, 2, 0))\n","    plt.title(title)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iINHxwYK4k0e"},"outputs":[],"source":["def create_dataset_and_loader(df, split_type, batch_size, transforms, num_workers=0, show_info=False, show_images=False):\n","    dataset = CustomDataset(df[df['Split_add_valid'] == split_type], transform=transforms[split_type])\n","    shuffle = True if split_type == 'train' else False\n","    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n","\n","    # データローダーの情報表示\n","    if show_info:\n","        print_dataloader_info(loader, f\"{split_type.capitalize()} Data Loader\")\n","\n","    # バッチ画像の表示\n","    if show_images:\n","        show_batch_with_title(loader, f\"{split_type.capitalize()} Batch Images\")\n","\n","    return loader"]},{"cell_type":"markdown","metadata":{"id":"c_shLGCMDM-Z"},"source":["# Model module"]},{"cell_type":"markdown","metadata":{"id":"IZFMMgeH4pee"},"source":["## Multi Head Attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6zFoZKBkSyx9"},"outputs":[],"source":["class SelfAttention(nn.Module):\n","    '''\n","    自己アテンション\n","    dim_hidden: 入力特徴量の次元\n","    num_heads : マルチヘッドアテンションのヘッド数\n","    qkv_bias  : クエリなどを生成する全結合層のバイアスの有無\n","    '''\n","    def __init__(self, dim_hidden: int, num_heads: int,\n","                 qkv_bias: bool=False):\n","        super().__init__()\n","\n","        # 特徴量を各ヘッドのために分割するので、\n","        # 特徴量次元をヘッド数で割り切れるか検証\n","        assert dim_hidden % num_heads  == 0\n","\n","        self.num_heads = num_heads\n","\n","        # ヘッド毎の特徴量次元\n","        dim_head = dim_hidden // num_heads\n","\n","        # ソフトマックスのスケール値\n","        self.scale = dim_head ** -0.5 # （2乗根の逆数、ViT入門では2乗根）\n","\n","        # ヘッド毎にクエリ、キーおよびバリューを生成するための全結合層\n","        self.proj_in = nn.Linear(\n","            dim_hidden, dim_hidden * 3, bias=qkv_bias) # 3はq, k, vの3つ分か\n","\n","        # 各ヘッドから得られた特徴量を一つにまとめる全結合層\n","        self.proj_out = nn.Linear(dim_hidden, dim_hidden)\n","\n","        # アテンションの勾配とマップ（アテンションの値（類似度））格納用\n","        self.attn_gradients = None\n","        self.attention_map = None\n","\n","    def save_attn_gradients(self, attn_gradients):\n","        self.attn_gradients = attn_gradients\n","\n","    def get_attn_gradients(self):\n","        return self.attn_gradients\n","\n","    def save_attention_map(self, attention_map):\n","        self.attention_map = attention_map\n","\n","    def get_attention_map(self):\n","        return self.attention_map\n","\n","    '''\n","    順伝播関数\n","    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n","    '''\n","    def forward(self, x: torch.Tensor, register_hook=False):\n","        # print(\"x.requires_grad:\", x.requires_grad)\n","\n","        bs, ns = x.shape[:2]\n","\n","        qkv = self.proj_in(x) # 線形変換 + 特徴量の数を3倍？\n","\n","        # view関数により\n","        # [バッチサイズ, 特徴量数, QKV, ヘッド数, ヘッドの特徴量次元]\n","        # permute関数により\n","        # [QKV, バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n","        qkv = qkv.view(\n","            bs, ns, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n","\n","        # クエリ、キーおよびバリューに分解\n","        q, k, v = qkv.unbind(0)\n","\n","        # クエリとキーの行列積とアテンションの計算（今回マスクは不使用）\n","        # attnは[バッチサイズ, ヘッド数, 特徴量数, 特徴量数]\n","        attn = q.matmul(k.transpose(-2, -1))\n","        attn = (attn * self.scale).softmax(dim=-1)\n","\n","        # attnテンソルが勾配を持つかどうか確認\n","        # print(\"attn.requires_grad:\", attn.requires_grad)\n","\n","        # アテンションマップを保存\n","        self.save_attention_map(attn)\n","\n","        # register_hookがTrueなら、アテンションの勾配を保存するフックを登録。\n","        # print(\"register_hook: \", register_hook)\n","        if register_hook:\n","            attn.register_hook(self.save_attn_gradients)\n","\n","        # アテンションとバリューの行列積によりバリューを収集\n","        # xは[バッチサイズ, ヘッド数, 特徴量数, ヘッドの特徴量次元]\n","        x = attn.matmul(v)\n","\n","        # permute関数により\n","        # [バッチサイズ, 特徴量数, ヘッド数, ヘッドの特徴量次元]\n","        # flatten関数により全てのヘッドから得られる特徴量を連結して、\n","        # [バッチサイズ, 特徴量数, ヘッド数 * ヘッドの特徴量次元]\n","        x = x.permute(0, 2, 1, 3).flatten(2)\n","        x = self.proj_out(x) # 線形変換\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"16HsYmUB7Y7k"},"source":["## FNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Kn0qqfnhFkN"},"outputs":[],"source":["class FNN(nn.Module):\n","    '''\n","    Transformerエンコーダ内の順伝播型ニューラルネットワーク\n","    dim_hidden     : 入力特徴量の次元\n","    dim_feedforward: 中間特徴量の次元\n","    '''\n","    def __init__(self, dim_hidden: int, dim_feedforward: int):\n","        super().__init__()\n","\n","        self.linear1 = nn.Linear(dim_hidden, dim_feedforward)\n","        self.linear2 = nn.Linear(dim_feedforward, dim_hidden)\n","        self.activation = nn.GELU()\n","\n","    '''\n","    順伝播関数\n","    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n","    '''\n","    def forward(self, x: torch.Tensor):\n","        x = self.linear1(x)\n","        x = self.activation(x)\n","        x = self.linear2(x)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"Ig9hE9Me717o"},"source":["## TransformerEncoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXm-XGjRkun6"},"outputs":[],"source":["class TransformerEncoderLayer(nn.Module):\n","    '''\n","    Transformerエンコーダ層\n","    dim_hidden     : 入力特徴量の次元\n","    num_heads      : ヘッド数\n","    dim_feedforward: 中間特徴量の次元\n","    '''\n","    def __init__(self, dim_hidden: int,\n","                 num_heads: int,\n","                 dim_feedforward: int):\n","        super().__init__()\n","\n","        self.attention = SelfAttention(dim_hidden, num_heads)\n","        self.fnn = FNN(dim_hidden, dim_feedforward)\n","\n","        self.norm1 = nn.LayerNorm(dim_hidden)\n","        self.norm2 = nn.LayerNorm(dim_hidden)\n","\n","    '''\n","    順伝播関数\n","    x: 入力特徴量, [バッチサイズ, 特徴量数, 特徴量次元]\n","    '''\n","    def forward(self, x: torch.Tensor, register_hook=False):\n","        x = self.norm1(x)\n","        x = self.attention(x, register_hook=register_hook) + x\n","        x = self.norm2(x)\n","        x = self.fnn(x) + x\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"91uUNEuL8Wq1"},"source":["## Vision Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R8zNwYKRrEen"},"outputs":[],"source":["class VisionTransformer(nn.Module):\n","    '''\n","    Vision Transformer\n","    num_classes    : 分類対象の物体クラス数\n","    img_size       : 入力画像の大きさ(幅と高さ等しいことを想定)\n","    patch_size     : パッチの大きさ(幅と高さ等しいことを想定)\n","    dim_hidden     : 入力特徴量の次元\n","    num_heads      : マルチヘッドアテンションのヘッド数\n","    dim_feedforward: FNNにおける中間特徴量の次元\n","    num_layers     : Transformerエンコーダの層数\n","    '''\n","    def __init__(self, num_classes: int,\n","                 img_size: int,\n","                 patch_size: int,\n","                 dim_hidden: int,\n","                 num_heads: int,\n","                 dim_feedforward: int,\n","                 num_layers: int):\n","        super().__init__()\n","\n","        # 画像をパッチに分解するために、画像の大きさがパッチの大きさで割り切れるか確認\n","        assert img_size % patch_size == 0\n","\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","\n","        # パッチの行数と列数はともにimg_size // patch_sizeであり、\n","        # パッチ数はその2乗になる\n","        num_patches = (img_size // patch_size) ** 2\n","\n","        # パッチ特徴量はパッチを平坦化することにより生成されるため\n","        # その次元はpatch_size * patch_size * 3（RGBチャネル）\n","        dim_patch = 3 * patch_size ** 2\n","\n","        # パッチ特徴量をTransformerエンコーダーに入力する前に\n","        # パッチ特徴量の次元を変換する全結合層\n","        self.patch_embed = nn.Linear(dim_patch, dim_hidden)\n","\n","        # 位置埋め込み（パッチ数 + クラス埋め込みの分を用意）\n","        self.pos_embed = nn.Parameter(\n","            torch.zeros(1, num_patches + 1, dim_hidden))\n","\n","        # クラス埋め込み\n","        self.class_token = nn.Parameter(\n","            torch.zeros((1, 1, dim_hidden))\n","        )\n","\n","        # Transformerエンコーダ層\n","        self.layers = nn.ModuleList([TransformerEncoderLayer(\n","            dim_hidden, num_heads, dim_feedforward\n","        ) for _ in range(num_layers)])\n","\n","        # ロジット（ニューロンの出力値）を生成する前のレイヤー正規化と全結合\n","        self.norm = nn.LayerNorm(dim_hidden)\n","        self.linear = nn.Linear(dim_hidden, num_classes)\n","\n","    '''\n","    順伝播関数\n","    x           ： 入力, [バッチサイズ, 入力チャネル数, 高さ, 幅]\n","    return_embed： 特徴量を返すかロジットを返すかを選択する真偽値\n","    '''\n","    def forward(self, x: torch.Tensor,\n","                return_embed: bool=False,\n","                register_hook=False):\n","        bs, c, h, w = x.shape\n","\n","        # 入力画像の大きさがクラス生成時に指定したimg_sizeと\n","        # 合致しているか確認\n","        assert h == self.img_size and w == self.img_size\n","\n","        # 高さ軸と幅軸をそれぞれパッチ数 * パッチの大きさに分解し、\n","        # [バッチサイズ, チャネル数, パッチの行数, パッチの大きさ,\n","        # パッチの列数, パッチの大きさ] の形にする\n","        x = x.view(bs, c, h // self.patch_size, self.patch_size,\n","                w // self.patch_size, self.patch_size)\n","\n","        # permute関数により、\n","        # [バッチサイズ, パッチ行数, パッチ列数, チャネル,\n","        #                   パッチの大きさ, パッチの大きさ] の形にする\n","        x = x.permute(0, 2, 4, 1, 3, 5)\n","\n","        # パッチを平坦化\n","        # permute関数適用後にはメモリ上のデータ配置の整合性の関係で\n","        # view関数を使えないのでreshape関数を使用\n","        x = x.reshape(\n","            bs, (h // self.patch_size) * (w // self.patch_size), -1)\n","\n","        x = self.patch_embed(x)\n","\n","        # クラス埋め込みをバッチサイズ分用意\n","        class_token = self.class_token.expand(bs, -1, -1)\n","\n","        x = torch.cat((class_token, x), dim=1)\n","\n","        x += self.pos_embed\n","\n","        # Transformerエンコーダ層を適用\n","        # print(\"register_hook\", register_hook)\n","        for layer in self.layers:\n","            x = layer(x, register_hook=register_hook)\n","\n","        # クラス埋め込みをベースとした特徴量を抽出\n","        x = x[:, 0]\n","\n","        x = self.norm(x)\n","\n","        if return_embed: # Trueならロジットを返す\n","            return x\n","\n","        x = self.linear(x)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"1SvFBHzH5xRx"},"source":["# LightningModule"]},{"cell_type":"markdown","metadata":{"id":"nw_IrV4vEu2H"},"source":["## data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dONqu2vywdyc"},"outputs":[],"source":["class PLDataModule(pl.LightningDataModule):\n","    def __init__(self, train_loader, val_loader, test_loader):\n","        super().__init__()\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","        self.test_loader = test_loader\n","\n","    def train_dataloader(self):\n","        return self.train_loader\n","\n","    def val_dataloader(self):\n","        return self.val_loader\n","\n","    def test_dataloader(self):\n","        return self.test_loader"]},{"cell_type":"markdown","metadata":{"id":"qSRwBlFXEzLl"},"source":["## lightning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PbTvBoLpg_2f"},"outputs":[],"source":["class PLViTModule(pl.LightningModule):\n","    def __init__(self, config):\n","        super(PLViTModule, self).__init__()\n","\n","        self.lr = config['lr']\n","        self.num_classes = config['num_classes']\n","        self.img_size = config['img_size']\n","        self.patch_size = config['patch_size']\n","        self.dim_hidden = config['dim_hidden']\n","        self.num_heads = config['num_heads']\n","        self.dim_feedforward = config['dim_feedforward']\n","        self.num_layers = config['num_layers']\n","        self.loss_fn = nn.CrossEntropyLoss()\n","        self.accuracy = Accuracy(num_classes=self.num_classes, task=\"multiclass\")\n","        self.class_accuracy = torch.nn.ModuleList([Accuracy(num_classes=self.num_classes, task=\"multiclass\") for _ in range(self.num_classes)])\n","        self.class_names = [STL10_CLASSES[i] for i in range(len(STL10_CLASSES))]\n","        self.save_test_predictions = config['save_test_predictions']\n","        self.test_predictions = []\n","\n","        # モデル定義\n","        self.models = VisionTransformer(num_classes = self.num_classes,\n","                                        img_size = self.img_size,\n","                                        patch_size = self.patch_size,\n","                                        dim_hidden = self.dim_hidden,\n","                                        num_heads = self.num_heads,\n","                                        dim_feedforward = self.dim_feedforward,\n","                                        num_layers = self.num_layers)\n","\n","        # ハイパーパラメーターをself.hparamsに保存する (W&Bによって自動ロギングされる)\n","        self.save_hyperparameters()\n","\n","    def forward(self, x: torch.Tensor, return_embed: bool = False, register_hook=False):\n","        output = self.models(x, register_hook=register_hook)\n","        return output\n","\n","    def training_step(self, batch, batch_idx):\n","        images, target = batch\n","        # print(images.shape, target)\n","        preds = self.forward(images)\n","        # クロスエントロピー、これでモデルを更新\n","        loss = self.loss_fn(preds, target)\n","\n","        # Accuracyの計算\n","        self.accuracy(preds, target)\n","        # validでは不要だが、training_stepではこうしないとwandbにaccuracyが保存されない\n","        current_accuracy = self.accuracy.compute()\n","\n","        # ログを記録(self.logはPyTorch Lightninのメソッド)\n","        self.log(\"01_loss/train\", loss, prog_bar=True, logger=True, on_epoch=True, on_step=False)\n","        self.log(\"02_metrics/accuracy_train\", current_accuracy, prog_bar=False, logger=True, on_epoch=True, on_step=False)\n","\n","        return {\"loss\": loss}  # このlossを基にモデルが更新される\n","\n","    def validation_step(self, batch, batch_idx):\n","        images, target = batch\n","        # print(images.shape, target)\n","        preds = self.forward(images)\n","        loss = self.loss_fn(preds, target)\n","        self.accuracy(preds, target)\n","\n","        # ログを記録\n","        self.log(\"01_loss/valid\", loss, prog_bar=True, logger=True, on_epoch=True, on_step=False)\n","        self.log(\"02_metrics/accuracy_valid\", self.accuracy, prog_bar=False, logger=True, on_epoch=True, on_step=False)\n","\n","        return {\"valid_loss\": loss}\n","\n","    def test_step(self, batch, batch_idx):\n","        images, targets = batch\n","        preds = self.forward(images)\n","        loss = self.loss_fn(preds, targets)\n","        self.accuracy(preds, targets)\n","\n","        # ログを記録\n","        self.log(\"01_loss/test\", loss, prog_bar=True, logger=True, on_epoch=True, on_step=False)\n","        self.log(\"02_metrics/accuracy_test\", self.accuracy, prog_bar=False, logger=True, on_epoch=True, on_step=False)\n","\n","        pred_labels = preds.argmax(dim=1)\n","        class_probabilities = preds.softmax(dim=1)\n","\n","        # 画像と予測結果を記録\n","        for i in range(images.size(0)):\n","            self.test_predictions.append({\n","                \"images\": images[i],\n","                \"pred_labels\": pred_labels[i],\n","                \"true_labels\": targets[i],\n","                \"class_probabilities\": class_probabilities[i],\n","            })\n","\n","        # ここで全体の精度を計算\n","        self.accuracy(preds, targets)\n","\n","        # クラスごとの精度を計算\n","        for i in range(self.num_classes):\n","            class_mask = targets == i\n","            class_preds = preds[class_mask]\n","            class_targets = targets[class_mask]\n","            if class_targets.nelement() != 0:  # クラスに対するデータがある場合のみ計算\n","                self.class_accuracy[i](class_preds, class_targets)\n","\n","        return {\"test_loss\": loss}\n","\n","    def on_test_epoch_end(self):\n","        if self.save_test_predictions:\n","          # wandbテーブルにカラムを追加（\"image\"カラムも含む）\n","          columns = [\"image\", \"pred\", \"truth\"] + self.class_names\n","          test_table = wandb.Table(columns=columns)\n","\n","          # test_predictionsに保存されたデータを使ってwandb.Tableを作成\n","          for output in self.test_predictions:\n","              # wandb.Imageを使用して画像データをテーブルに追加\n","              image_data = wandb.Image(output[\"images\"].cpu())  # GPU上の画像データをCPUに移動\n","              pred_label_index = output[\"pred_labels\"].item()\n","              true_label_index = output[\"true_labels\"].item()\n","\n","              # クラスインデックスをクラス名に変換\n","              pred_label_name = STL10_CLASSES[pred_label_index]\n","              true_label_name = STL10_CLASSES[true_label_index]\n","\n","              class_probabilities = output[\"class_probabilities\"].tolist()\n","\n","              # wandbテーブルにデータを追加\n","              row = [image_data, pred_label_name, true_label_name] + class_probabilities\n","              test_table.add_data(*row)\n","\n","          # wandbにテーブルをログとして保存（wandb.logは、カスタムデータ（画像、テーブル、グラフなど）をWandBに記録するのに特化）\n","          wandb.log({\"Test Predictions\": test_table})\n","\n","          # テスト予測データをクリア\n","          self.test_predictions.clear()\n","\n","        # 全体の精度を計算\n","        overall_accuracy = self.accuracy.compute()\n","        # クラスごとの精度をリストに保存\n","        class_accuracies = [self.class_accuracy[i].compute() for i in range(self.num_classes)]\n","\n","        # Weights & Biases の棒グラフを作成するためのデータを準備\n","        data = [[\"Overall\", overall_accuracy]] + [[self.class_names[i], class_accuracies[i]] for i in range(len(self.class_names))]\n","\n","        # Weights & Biases 用のテーブルオブジェクトを作成\n","        accuracy_table = wandb.Table(data=data, columns=[\"Class\", \"Accuracy\"])\n","\n","        # 棒グラフを作成してログに記録\n","        wandb.log({\"Class-wise Test Accuracies\": wandb.plot.bar(accuracy_table, \"Class\", \"Accuracy\", title=\"Class-wise Test Accuracies\")})\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.models.parameters(), lr=self.lr)\n","        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n","        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}"]},{"cell_type":"markdown","metadata":{"id":"QTPb5XTE9C3M"},"source":["## callback"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDRH9btJ7AVW"},"outputs":[],"source":["class PLCallback(Callback):\n","    def __init__(self,\n","                 config,\n","                 num_samples=32,\n","                 epoch_interval=10,\n","                 tsne_img_path=None,\n","                 data_module=None):\n","\n","        self.channel_mean = config['channel_mean']\n","        self.channel_std = config['channel_std']\n","        self.tsne_samples = config['tsne_samples']\n","\n","        self.num_samples = num_samples  # ログに記録するサンプル数（バッチサイズ以上にすること）\n","        self.epoch_interval = epoch_interval  # エポックの間隔を指定\n","        self.tsne_img_path = tsne_img_path\n","        self.data_module = data_module\n","\n","    def on_validation_epoch_end(self, trainer, pl_module):\n","        # epoch_intervalごとに処理を行う\n","        if trainer.current_epoch % self.epoch_interval == 0:\n","            # モデルの取得\n","            model = pl_module\n","            # 検証データローダー\n","            dataloader = trainer.val_dataloaders\n","\n","            \"\"\"サンプル画像と正解ラベルを取得\"\"\"\n","            batch = next(iter(dataloader))\n","            x, y = batch\n","\n","            device = model.device\n","            x, y = x.to(device), y.to(device)\n","            x.requires_grad_()  # 勾配を追跡\n","\n","            # サンプル数に応じてデータを取得\n","            images = [img for img in x[:self.num_samples]]\n","\n","            def denormalize(image, mean, std):\n","                mean = torch.tensor(mean).view(3, 1, 1).to(image.device)\n","                std = torch.tensor(std).view(3, 1, 1).to(image.device)\n","                return image * std + mean\n","\n","            # 元画像の逆正規化\n","            images = [denormalize(img, self.channel_mean, self.channel_std) for img in images]\n","\n","            \"\"\"t-SNEとTE用にモデルを保存してロード\"\"\"\n","            vee_model = copy.deepcopy(pl_module)\n","\n","            te_images_with_captions = []\n","\n","            \"\"\"Transformer-Explainability\"\"\"\n","            # 予測値の取得\n","            preds = [vee_model(img.unsqueeze(0).cuda()) for img in images]\n","\n","            with torch.set_grad_enabled(True):\n","                # TE実行\n","                te = Transformer_Explainability.Transformer_Explainability(model=vee_model, cls_to_idx=STL10_CLASSES)\n","                # サンプル画像の枚数分行う\n","                for original_img, pred in zip(images, preds):\n","                    # te_imageは0-255の範囲\n","                    te_image = te.generate_visualization(original_img)\n","\n","                    # te_imageをテンソルに変換し、次元の順序を変更し、範囲を0.0〜1.0に変更\n","                    te_image_tensor = torch.tensor(te_image).to(original_img.device).float() / 255.0\n","                    te_image_tensor = te_image_tensor.permute(2, 0, 1)\n","\n","                    # 元の画像とte_imageを横に並べて結合\n","                    combined_image = torch.cat((original_img, te_image_tensor), dim=2)  # dim=2は横方向に結合するため\n","\n","                    # wandbはnumpy配列を受け取るので、テンソルをnumpy配列に変換\n","                    combined_image_np = combined_image.detach().cpu().numpy().transpose(1, 2, 0)  # CHW -> HWC\n","\n","                    # 予測結果の文字列取得\n","                    top_str = te.print_top_classes(pred)\n","                    # wandb保存用のリスト\n","                    te_images_with_captions.append((combined_image_np, top_str))\n","\n","            # 画像とそのキャプションを組み合わせてwandbにログ\n","            wandb.log({\"te_images\": [wandb.Image(image, caption=caption) for image, caption in te_images_with_captions]})\n","\n","            \"\"\"t-SNE\"\"\"\n","            util.plot_t_sne(self.data_module.test_dataloader(),\n","                            vee_model,\n","                            self.tsne_samples,\n","                            vee_model.device,\n","                            self.tsne_img_path)\n","\n","            # t-SNEプロットをログ\n","            wandb.log({\"t-SNE Plot (Valid End ※テストデータ使用)\": [wandb.Image(plt.imread(self.tsne_img_path))]})"]},{"cell_type":"markdown","metadata":{"id":"CzabO8UEZfS2"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLwvqFKThmpq"},"outputs":[],"source":["def train(config):\n","    seed_everything(config[\"seed\"])\n","\n","    # cudnnの決定性を保証（精度下がる可能性あり）\n","    # torch.backends.cudnn.deterministic = True\n","    # torch.backends.cudnn.benchmark = False\n","\n","    # wandbの初期化\n","    run = wandb.init(project=WANDB_PROJECT,\n","                 entity=None,\n","                 job_type=WANDB_JOBTYPE,\n","                 group=WANDB_GROUP,\n","                 # name=WANDB_NAME,\n","                 config=config)\n","\n","    # PyTorchLightningとwandbを統合\n","    wandb_logger = WandbLogger(project=WANDB_PROJECT,\n","                               config=config,\n","                               job_type=WANDB_JOBTYPE,\n","                               )\n","\n","    # データのダウンロードとDataFrameの取得\n","    processed_dataset_dir, processed_dataset_dir_v0 = download_data()\n","    df = get_df(config, processed_dataset_dir, processed_dataset_dir_v0, data_reduction=config[\"data_reduction\"])\n","\n","    channel_mean, channel_std = config[\"channel_mean\"], config[\"channel_std\"]\n","    transforms = create_transforms(config[\"img_size\"], channel_mean, channel_std)\n","\n","    train_loader = create_dataset_and_loader(df, 'train', config[\"batch_size\"], transforms, num_workers=config['num_workers'], show_info=True, show_images=False)\n","    val_loader = create_dataset_and_loader(df, 'valid', config[\"batch_size\"], transforms, num_workers=config['num_workers'], show_info=True, show_images=False)\n","    test_loader = create_dataset_and_loader(df, 'test', config[\"batch_size\"], transforms, num_workers=config['num_workers'], show_info=True, show_images=False)\n","\n","    data_module = PLDataModule(train_loader, val_loader, test_loader)\n","    model = PLViTModule(config)\n","\n","    # wandb.watchの実行有無を設定\n","    if config.get(\"use_wandb_watch\", False):\n","        wandb_logger.watch(model, log=\"all\", log_freq=config[\"log_freq\"])\n","\n","    # callback時のt-SNEプロットの保存先\n","    tsne_img_path = config[\"val_epoch_end_dir\"] + \"vee_tsne.png\"\n","\n","    # val_epoch_endディレクトリの存在確認と作成\n","    val_epoch_end_dir = config[\"val_epoch_end_dir\"]\n","    if not os.path.exists(val_epoch_end_dir):\n","        os.makedirs(val_epoch_end_dir)\n","\n","    # checkpointディレクトリの存在確認と作成\n","    checkpoint_dir = config[\"save_dir\"]\n","    if not os.path.exists(checkpoint_dir):\n","        os.makedirs(checkpoint_dir)\n","\n","    model_checkpoint = ModelCheckpoint(\n","        filename = WANDB_GROUP + \"_\" + run.name + \"_{epoch}\", # モデルファイルの名前\n","        dirpath = config[\"save_dir\"],           # モデルを保存するディレクトリのパス\n","        monitor = \"01_loss/valid\",              # 監視する指標\n","        mode = \"min\",                           # 最小化を目指す\n","        save_top_k = 1,                         # 最良のK個のモデルを保存\n","        save_last = False,                      # 最後のエポックのモデルを保存\n","    )\n","\n","    early_stopping = EarlyStopping(\n","        monitor=\"01_loss/valid\",\n","        mode=\"min\",\n","        patience=config[\"patience\"],\n","    )\n","\n","    # callbackの初期化\n","    callbacks = [model_checkpoint, early_stopping]\n","\n","    # PLCallbackを使用するかどうかの設定\n","    if config.get(\"use_pl_callback\", False):\n","        pl_callback = PLCallback(\n","            config,\n","            num_samples=32,\n","            epoch_interval=config[\"val_epoch_interval\"],\n","            tsne_img_path=tsne_img_path,\n","            data_module=data_module\n","        )\n","        callbacks.append(pl_callback)\n","\n","    # Trainer\n","    trainer = pl.Trainer(\n","        max_epochs=config['num_epochs'],\n","        accelerator=\"auto\",\n","        precision=config['precision'],\n","        callbacks=callbacks,\n","        logger=[wandb_logger],\n","        deterministic=True,  # モデルの再現性確保（非決定的な機能を制御）\n","    )\n","\n","    # 学習実行\n","    trainer.fit(\n","        model,\n","        datamodule=data_module,\n","    )\n","\n","    # テスト実行\n","    if config.get(\"run_test\", False):\n","        ## ベストモデルのパラメータ（重み）を自動的に読み込む\n","        trainer.test(dataloaders=data_module.test_dataloader())\n","\n","    # Close wandb run\n","    wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"7nNQXVNQsfp1"},"source":["# Config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLYhs96Ty42r"},"outputs":[],"source":["base_dir = r\"drive/MyDrive/wandb_mlops/\"\n","\n","'''\n","Weights and Biasesの設定\n","'''\n","WANDB_PROJECT = \"mlops-001\"           # プロジェクトの名前\n","WANDB_GROUP = \"ViT\"                   # グループの名前\n","# WANDB_NAME = \"Sweeps_best\"     # 学習時の名前\n","WANDB_JOBTYPE = \"train\"\n","RAW_DATA_AT = 'STL10'\n","PROCESSED_DATA_AT = 'STL10_split'\n","STL10_CLASSES = {i:c for i,c in enumerate(['airplane', 'bird', 'car', 'cat', 'deer', 'dog', 'horse', 'monkey', 'ship', 'truck'])}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_GBub_7msnms"},"outputs":[],"source":["# default\n","train_config = dict(\n","    save_dir = base_dir + \"checkpoints/\",               # モデル保存先\n","    val_epoch_end_dir = base_dir + \"val_epoch_end/\",    # on_validation_epoch_endで使うモデルの保存先\n","\n","    data_reduction = False,                 # データをbatch_size*3に削減する\n","    use_pl_callback = False,                # PLCallbackの使用有無（Transformer-Explainability, t-SNE）\n","    use_wandb_watch = False,                # wandb.watchの使用有無\n","    run_test = True,                        # testの実行有無\n","    save_test_predictions = False,           # テスト予測データの保存有無\n","\n","    num_epochs = 50,                        # 学習エポック数 30\n","    val_epoch_interval = 10,                # on_validation_epoch_end（TEとt-SNE）を実行するエポックの間隔\n","    patience = 20,                          # early_stoppingのpatience\n","    batch_size = 128,                       # バッチサイズ\n","\n","    num_classes = 10,                       # データセットのクラス数\n","    num_workers = 4,                        # データローダに使うCPUプロセスの数\n","    tsne_samples = 1000,                    # t-SNEでプロットするサンプル数\n","    precision = 16,                         # 32\n","\n","    lr = 1e-4,                              # 学習率\n","    img_size = 96,                          # 入力画像の大きさ\n","    patch_size = 8,                         # パッチサイズ\n","    dim_hidden = 512,                       # 隠れ層の次元\n","    num_heads = 8,                          # マルチヘッドアッテンションのヘッド数\n","    dim_feedforward = 512,                  # Transformerエンコーダ層内のFNNにおける隠れ層の特徴量次元\n","    num_layers = 6,                         # Transformerエンコーダの層数\n","    seed=42,\n","    log_freq = 100,                          # wandb.watch()で保存するstepの間隔（デフォルト:100）\n","    channel_mean = [0.4467, 0.4398, 0.4066], # STL10データセットの平均\n","    channel_std = [0.2603, 0.2566, 0.2713]   # STL10データセットの標準偏差\n",")"]},{"cell_type":"markdown","metadata":{"id":"ulHi8ICpMg5y"},"source":["# Run"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":87688,"status":"ok","timestamp":1702711593020,"user":{"displayName":"nota tech","userId":"08868539642021101661"},"user_tz":-540},"id":"1s6vPUxARPJo","outputId":"cdd8e27c-fbf6-4707-bbea-4f1cdd0fd7ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Sat Dec 16 07:26:32 2023       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0              43W / 400W |      2MiB / 40960MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n","Your runtime has 89.6 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}],"source":["# Google Driveのマウント\n","mount(gpu_info=True, ram_info=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2WKXFRYW1Yg"},"outputs":[],"source":["import sys\n","sys.path.append(base_dir)\n","\n","# t-SNE\n","import util\n","# Transformer_Explainability\n","import Transformer_Explainability"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17618,"status":"ok","timestamp":1702711627911,"user":{"displayName":"nota tech","userId":"08868539642021101661"},"user_tz":-540},"id":"O_E3gBgBb0vs","outputId":"96d0b098-d27f-43ba-a369-116b84679d9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}],"source":["# Weights and Biases\n","import wandb\n","!wandb login"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3_7Bs_TQTDY6"},"outputs":[],"source":["# train(train_config)"]},{"cell_type":"markdown","metadata":{"id":"VDR9U2WuTONS"},"source":["# Sweep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88uncdnwMMsg"},"outputs":[],"source":["def train_wrapper():\n","    with wandb.init() as run:\n","        wandb_config = run.config\n","\n","        # 既存のtrain_configのコピーを作成\n","        train_config_updated = train_config.copy()\n","        # Sweepで変更されるすべてのパラメータを更新\n","        train_config_updated['lr'] = wandb_config.lr\n","        train_config_updated['batch_size'] = wandb_config.batch_size\n","        train_config_updated['dim_hidden'] = wandb_config.dim_hidden\n","        train_config_updated['num_layers'] = wandb_config.num_layers\n","        train_config_updated['patch_size'] = wandb_config.patch_size\n","        train_config_updated['num_heads'] = wandb_config.num_heads\n","\n","        # 更新された設定でトレーニング関数を実行\n","        train(train_config_updated)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eb40cSiQNSH3"},"outputs":[],"source":["sweep_config = {\n","    'method': 'random',  # サーチ手法（例：'random', 'grid', 'bayes'）\n","    'metric': {\n","        'name': '01_loss/valid',  # 最適化したいメトリック\n","        'goal': 'minimize'   # 目標（'minimize' または 'maximize'）\n","    },\n","    'parameters': {\n","        # 'lr': {\n","        #     'min': 1e-6,\n","        #     'max': 5e-4,\n","        #     'distribution': 'uniform'\n","        # },\n","        'lr': {\n","            'values': [1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4]\n","        },\n","        'batch_size': {\n","            'values': [32, 64, 128]\n","        },\n","        'dim_hidden': {\n","            'values': [256, 512, 768, 1024]  # 隠れ層の次元の範囲\n","        },\n","        'num_layers': {\n","            'values': [3, 6, 9, 12]  # 層数の範囲\n","        },\n","        'patch_size': {\n","            'values': [4, 8]  # パッチサイズの範囲\n","        },\n","        'num_heads': {\n","            'values': [4, 8, 16]  # ヘッド数の範囲\n","        }\n","    },\n","    'early_terminate': {\n","        'type': 'hyperband',   # early_terminateのタイプ\n","        'min_iter': 20,        # 最小イテレーション数\n","        'eta': 3               # etaパラメータ（通常は3）\n","    },\n","    'run_cap': 50  # 最大試行回数\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9nYo3ZWjIGrH","outputId":"c01a7e0b-db65-4432-8e61-6c2f4e5c2613","executionInfo":{"status":"ok","timestamp":1702718818867,"user_tz":-540,"elapsed":32931,"user":{"displayName":"nota tech","userId":"08868539642021101661"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Create sweep with ID: f693gxzy\n","Sweep URL: https://wandb.ai/56agumi85ten2sun/mlops-001/sweeps/f693gxzy\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"]}],"source":["# Sweepの作成\n","sweep_id = wandb.sweep(sweep_config, project=WANDB_PROJECT)\n","# Sweepエージェントの実行\n","wandb.agent(sweep_id, train_wrapper)"]},{"cell_type":"markdown","metadata":{"id":"vprj_mhKtRAO"},"source":["# End"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","toc_visible":true,"provenance":[],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}